{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, LeaveOneOut, cross_val_predict, KFold, GridSearchCV\n",
    "from sklearn.inspection import permutation_importance\n",
    "from pickle import dump\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust these to address the current train data and the path where the model needs to be saved + the name\n",
    "DATA_PATH = \"../data/train_data/INSRTR_SAR.csv\"\n",
    "MODEL_PATH = \"../insrtr/models/gbt_classifier_v3.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categories(dataframe_to_encode):\n",
    "    \"\"\"\n",
    "    Encodes categories using cat.codes from scikit.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe_to_encode: input dataframe\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe_to_encode: modified dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    cols_to_encode = [\"Enzyme\", \"resi_type\", \"resi_dssp\", \"prev_resi_type\", \"prev_resi_dssp\", \"next_resi_type\",\n",
    "                      \"next_resi_dssp\", \"loop_seq\"]\n",
    "    encoded_cols = [col + \"_encoded\" for col in cols_to_encode]\n",
    "    # Apply astype and cat.codes to each column\n",
    "    encoded = dataframe_to_encode[cols_to_encode].apply(lambda column: column.astype(\"category\").cat.codes)\n",
    "    # Use assign to create new columns in dataframe\n",
    "    dataframe_to_encode = dataframe_to_encode.assign(**dict(zip(encoded_cols, encoded.T.values)))\n",
    "    return dataframe_to_encode\n",
    "\n",
    "\n",
    "def create_x_y(dataframe_to_split, target=\"\"):\n",
    "    \"\"\"\n",
    "    Splits the data into descriptive space (x) and target space (y).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe_to_split: input dataframe\n",
    "    target: string that represents the target column\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_array: descriptive space numpy array\n",
    "    y_array: target space numpy array\n",
    "\n",
    "    \"\"\"\n",
    "    x_array = dataframe_to_split.drop(\n",
    "        [\"max_fold_decrease_with_peptide\", \"%WT_activity\", \"works\", \"fluctuation\", \"relative_MSA_conservation\",\n",
    "         \"Enzyme_encoded\", \"Enzyme\", \"resi_type\", \"resi_dssp\", \"prev_resi_type\", \"prev_resi_dssp\",\n",
    "         \"next_resi_type\", \"next_resi_dssp\", \"loop_seq\"], axis=1).iloc[:, :].copy().values\n",
    "    y_array = dataframe_to_split[target].values.ravel()\n",
    "    return x_array, y_array\n",
    "\n",
    "\n",
    "def save_model(classifier, filename):\n",
    "    \"\"\"\n",
    "    Saves a scikit-learn model to disk using the pickle module.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    classifier: trained model that should be saved\n",
    "    filename: where the pickle file is saved\n",
    "    \"\"\"\n",
    "    with open(filename, \"wb\") as file:\n",
    "        dump(classifier, file)\n",
    "\n",
    "\n",
    "def get_feature_importance(original_dataframe, classifier, top_n=15):\n",
    "    \"\"\"\n",
    "    Returns N most important features, as calculated by the permutation importance.\n",
    "    The permutation feature importance is defined to be the decrease in a model score when a single feature value is randomly shuffled.\n",
    "    The more the score decreases, the more important the feature is.\n",
    "    Permutation importance does not reflect to the intrinsic predictive value of a feature by itself but how important this feature is for a particular model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    original_dataframe: the dataframe with original features on which the model was trained\n",
    "    classifier: trained model\n",
    "    top_n: number of features to be returned\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    f_imp: dataframe with top_n most important features for the model\n",
    "    \"\"\"\n",
    "    xdf =  original_dataframe.drop(\n",
    "        [\"max_fold_decrease_with_peptide\", \"%WT_activity\", \"works\", \"fluctuation\", \"relative_MSA_conservation\",\n",
    "         \"Enzyme_encoded\", \"Enzyme\", \"resi_type\", \"resi_dssp\", \"prev_resi_type\", \"prev_resi_dssp\",\n",
    "         \"next_resi_type\", \"next_resi_dssp\", \"loop_seq\"], axis=1).iloc[:, :].copy()\n",
    "    x_arr = xdf.values\n",
    "    y_arr = original_dataframe['works'].values\n",
    "    result = permutation_importance(classifier, x_arr, y_arr, n_repeats=20, random_state=42)\n",
    "    f_imp =  pd.DataFrame({\"feature_name\": xdf.columns, \"importance\": result.importances_mean})\n",
    "    f_imp = f_imp.nlargest(top_n, \"importance\")\n",
    "    return f_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of approval for learning_rate=4: 55.32%\n",
      "Percentage of approval for min_samples=3: 61.70%\n",
      "LOOCV score on entire dataset: 0.6596\n",
      "Confusion matrix: \n",
      " [[17  6]\n",
      " [10 14]]\n",
      "AUC score: 0.7083\n"
     ]
    }
   ],
   "source": [
    "# Read data, drop embeddings and dummy columns, filer out rows with missing target\n",
    "df = pd.read_csv(DATA_PATH, usecols=lambda col: \"esm2\" not in col and \"Unnamed\" not in col).dropna(\n",
    "    subset=[\"works\"])\n",
    "df = encode_categories(df)\n",
    "\n",
    "# Separate the entire space into descriptive and target arrays\n",
    "x, y = create_x_y(df, \"works\")\n",
    "\n",
    "# Hyperparameter tuning scenario.\n",
    "# For each fold in the leave one out cross validation, do grid search on the training dataset.\n",
    "# The hyperparameter value is chosen based on maority vote.\n",
    "# Initialize the lists for the grid search\n",
    "best_params_gridsearch = []\n",
    "scores_cv =[]\n",
    "# Hyperparameters to be tuned with grid search\n",
    "parameters = {\"n_estimators\":[400,800],\n",
    "              \"learning_rate\":[0.1, 1, 2, 4],\n",
    "              \"min_samples_split\":[3, 5]}\n",
    "# Initialize model and fold\n",
    "clf = GradientBoostingClassifier(random_state=42)\n",
    "kf = KFold(n_splits=len(x), shuffle=True, random_state=42)\n",
    "cv = LeaveOneOut()\n",
    "clf_best = GridSearchCV(clf, parameters, cv=cv)\n",
    "# Apply the grid search in each LOOCV iteration\n",
    "for train_index, test_index in kf.split(x):\n",
    "    x_train, x_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    clf_best.fit(x_train, y_train)\n",
    "    best_params_gridsearch.append(clf_best.best_params_)\n",
    "    scores_cv.append(clf_best.best_estimator_.score(x_test, y_test))\n",
    "\n",
    "# Get the majority vote for the parameters\n",
    "learning_rate_gs = Counter([d[\"learning_rate\"] for d in best_params_gridsearch])\n",
    "learning_rate = learning_rate_gs.most_common(1)[0][0]\n",
    "learning_rate_percentage_approval = (learning_rate_gs.most_common(1)[0][1]/47)*100\n",
    "print(\"Percentage of approval for learning_rate={}: {:.2f}%\".format(learning_rate, learning_rate_percentage_approval))\n",
    "\n",
    "min_samples_split_gs = Counter([d[\"min_samples_split\"] for d in best_params_gridsearch])\n",
    "min_samples_split = min_samples_split_gs.most_common(1)[0][0]\n",
    "min_samples_split_percentage_approval = (min_samples_split_gs.most_common(1)[0][1]/47)*100\n",
    "print(\"Percentage of approval for min_samples={}: {:.2f}%\".format(min_samples_split, min_samples_split_percentage_approval))\n",
    "\n",
    "# Initialize the model with the best chosen parameters\n",
    "clf = GradientBoostingClassifier(n_estimators=800, random_state=42, learning_rate=learning_rate, min_samples_split=min_samples_split)\n",
    "scores = cross_val_score(clf, x, y, scoring=\"accuracy\", cv=cv, n_jobs=-1)\n",
    "avg_score = scores.mean()\n",
    "print(\"LOOCV score on entire dataset: {:.4f}\".format(avg_score))\n",
    "\n",
    "# Get predictions from LOOCV - label and probability, calculate AUC and show confusion matrix\n",
    "y_pred_label = cross_val_predict(clf, x, y, cv=cv, n_jobs=-1)\n",
    "conf_mat = confusion_matrix(y, y_pred_label, labels=[\"N\", \"Y\"])\n",
    "print(\"Confusion matrix: \\n\", conf_mat)\n",
    "y_pred_proba = cross_val_predict(clf, x, y, cv=cv, method=\"predict_proba\", n_jobs=-1)\n",
    "y_pred_proba = y_pred_proba[:, 1]\n",
    "auc_score = roc_auc_score(y, y_pred_proba)\n",
    "print(\"AUC score: {:.4f}\".format(auc_score))\n",
    "\n",
    "# Retrain model on entire set and save it as pickle file\n",
    "clf.fit(x, y)\n",
    "save_model(clf, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>resi_active_site_num_E_max</td>\n",
       "      <td>0.291489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>loop_sasa_A_per_res</td>\n",
       "      <td>0.172340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>resi_active_site_num_H_avg</td>\n",
       "      <td>0.031915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>loop_radius_gyration_A</td>\n",
       "      <td>0.031915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>resi_burial_percent</td>\n",
       "      <td>0.029787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>loop_seq_encoded</td>\n",
       "      <td>0.024468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>loop_P_percent</td>\n",
       "      <td>0.009574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>next_resi_type_encoded</td>\n",
       "      <td>0.008511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resi_loop_index0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>loop_index0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  feature_name  importance\n",
       "20  resi_active_site_num_E_max    0.291489\n",
       "28         loop_sasa_A_per_res    0.172340\n",
       "16  resi_active_site_num_H_avg    0.031915\n",
       "26      loop_radius_gyration_A    0.031915\n",
       "7          resi_burial_percent    0.029787\n",
       "42            loop_seq_encoded    0.024468\n",
       "33              loop_P_percent    0.009574\n",
       "40      next_resi_type_encoded    0.008511\n",
       "0             resi_loop_index0    0.000000\n",
       "1                  loop_index0    0.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get feature importance\n",
    "feature_importance = get_feature_importance(df, clf, top_n=10)\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
